{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d4490a",
   "metadata": {},
   "source": [
    "## Data Acquisition Crawler\n",
    "\n",
    "### **Main Objective**\n",
    "To automate the massive and organized download of thousands of legal judgments (focusing on *Texas Criminal Law*) from the public static database *Case.law*.\n",
    "\n",
    "### **Technical Logic**\n",
    "The script is designed to handle the acquisition of large volumes of data through a hierarchical and resilient structure:\n",
    "\n",
    "* **Hierarchical Navigation:** The crawler replicates the original database structure by navigating through three levels: *State* → *Volumes* → *Individual Cases*. It uses **BeautifulSoup** to parse the HTML of index pages and dynamically extract links to JSON files.\n",
    "* **Parallel Execution (Multithreading):** To speed up the download of thousands of small JSON files, the script uses `ThreadPoolExecutor` with 8 simultaneous workers. This allows downloading 8 cases at a time instead of sequentially, significantly reducing network wait times (I/O bound).\n",
    "* **Resilience and Idempotency:**\n",
    "    * **Retry Logic:** It includes a `fetch_url` function that retries up to 4 times in case of connection failure, ensuring that a momentary network error does not block the entire process.\n",
    "    * **Existence Check:** Before downloading, it checks if the file already exists locally (`if os.path.exists`). This allows the process to be interrupted and resumed (resume capability) without duplicating work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0b3fde-26dd-4560-b1ed-7f5acbb5b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202bbe7a-db05-4763-8baf-cd0c3110715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = [\"tex-crim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabb1237-f62f-47b8-94c7-b6b339ec0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://static.case.law\"\n",
    "DOWNLOAD_FOLDER = \"Test/case_law_json\"\n",
    "RETRIES = 4\n",
    "SLEEP_BETWEEN_RETRIES = 2  # seconds\n",
    "MAX_WORKERS = 8  # Number of threads for parallel downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c210407c-c107-462e-8276-7efe5a274062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url):\n",
    "    \"\"\"Esegue il fetch di un URL con tentativi (retries).\"\"\"\n",
    "    for attempt in range(RETRIES):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Lancia un errore per status HTTP > 400\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}. Retry {attempt + 1}/{RETRIES}\")\n",
    "            time.sleep(SLEEP_BETWEEN_RETRIES)\n",
    "    print(f\" Failed after {RETRIES} retries: {url}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_json(url, path):\n",
    "    \"\"\"Salva un file JSON da un URL, se non esiste già.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        # print(f\" Skipping {path} (already downloaded)\")\n",
    "        return True  # Già scaricato\n",
    "    \n",
    "    response = fetch_url(url)\n",
    "    if response is None:\n",
    "        print(f\" Failed to download {url}\")\n",
    "        return False\n",
    "    \n",
    "    # Crea la directory genitore se non esiste\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # print(f\" Saved {url} -> {path}\") # Riduciamo il rumore nel log\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_volumes(state):\n",
    "    \"\"\"Estrae i numeri dei volumi e gli URL dalla pagina indice dello stato.\"\"\"\n",
    "    state_url = f\"{BASE_URL}/{state}/\"\n",
    "    print(f\"Fetching volumes from: {state_url}\")\n",
    "    response = fetch_url(state_url)\n",
    "    if response is None:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    volumes = []\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        first_td = row.find(\"td\")\n",
    "        if first_td:\n",
    "            a = first_td.find(\"a\")\n",
    "            # Assicurati che il link sia un numero (volume)\n",
    "            if a and a.text.strip().isdigit():\n",
    "                vol_num = a.text.strip()\n",
    "                vol_url = a[\"href\"]\n",
    "                \n",
    "                # Costruisci l'URL completo se è relativo\n",
    "                if not vol_url.startswith(\"http\"):\n",
    "                    # L'URL corretto dovrebbe puntare alla cartella, non al file\n",
    "                    vol_url = f\"{BASE_URL}/{state}/{vol_num}/\" \n",
    "                \n",
    "                volumes.append((vol_num, vol_url))\n",
    "    print(f\"Found {len(volumes)} volumes for state {state}.\")\n",
    "    return volumes\n",
    "\n",
    "\n",
    "def get_cases(volume_url):\n",
    "    \"\"\"Estrae tutti gli URL dei JSON dei casi dalla pagina /cases/.\"\"\"\n",
    "    cases_url = f\"{volume_url}cases/\"\n",
    "    response = fetch_url(cases_url)\n",
    "    if response is None:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    case_urls = []\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        first_td = row.find(\"td\")\n",
    "        if first_td:\n",
    "            a = first_td.find(\"a\")\n",
    "            # Assicurati che il link finisca con .json\n",
    "            if a and a[\"href\"].endswith(\".json\"):\n",
    "                href = a[\"href\"]\n",
    "                if not href.startswith(\"http\"):\n",
    "                    href = cases_url + href\n",
    "                case_urls.append(href)\n",
    "    return case_urls\n",
    "\n",
    "\n",
    "def crawl_state(state):\n",
    "    \"\"\"Esegue il crawl completo per un singolo stato.\"\"\"\n",
    "    print(f\"=== Processing state: {state} ===\")\n",
    "\n",
    "    # Download dei metadati a livello di stato\n",
    "    state_metadata_files = [\"ReporterMetadata.json\", \"VolumesMetadata.json\"]\n",
    "    for meta in state_metadata_files:\n",
    "        url = f\"{BASE_URL}/{state}/{meta}\"\n",
    "        path = os.path.join(DOWNLOAD_FOLDER, state, meta)\n",
    "        save_json(url, path)\n",
    "\n",
    "    volumes = get_volumes(state)\n",
    "    \n",
    "    # Processa i volumi in parallelo (download dei metadati E dei casi)\n",
    "    for vol, vol_url in tqdm(volumes, desc=f\"Processing {state} volumes\"):\n",
    "        \n",
    "        # Download dei metadati a livello di volume\n",
    "        volume_metadata_files = [\"VolumeMetadata.json\", \"CasesMetadata.json\"]\n",
    "        for meta in volume_metadata_files:\n",
    "            url = f\"{vol_url}{meta}\"\n",
    "            path = os.path.join(DOWNLOAD_FOLDER, state, vol, meta)\n",
    "            save_json(url, path)\n",
    "\n",
    "        # Download di tutti i JSON dei casi per questo volume in parallelo\n",
    "        case_urls = get_cases(vol_url)\n",
    "        futures = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            for case_url in case_urls:\n",
    "                case_filename = case_url.split(\"/\")[-1]\n",
    "                save_path = os.path.join(DOWNLOAD_FOLDER, state, vol, \"cases\", case_filename)\n",
    "                \n",
    "                # Modifica: assicurati che la cartella \"cases\" esista\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                \n",
    "                futures.append(executor.submit(save_json, case_url, save_path))\n",
    "\n",
    "            # Mostra una barra di avanzamento per i download dei casi\n",
    "            for f in tqdm(as_completed(futures), total=len(futures), desc=f\"  Volume {vol} cases\", leave=False):\n",
    "                f.result() # Puoi controllare il risultato (True/False) se necessario\n",
    "\n",
    "    print(f\"=== Finished state: {state} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc342729-4ad5-468b-a7ca-6ccba6753f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing state: tex-crim ===\n",
      "Fetching volumes from: https://static.case.law/tex-crim/\n",
      "Found 142 volumes for state tex-crim.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tex-crim volumes: 100%|██████████| 142/142 [02:04<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finished state: tex-crim ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for state in STATES:\n",
    "        crawl_state(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
