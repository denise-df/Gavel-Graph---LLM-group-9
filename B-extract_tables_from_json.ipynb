{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68396e32",
   "metadata": {},
   "source": [
    "## Data Parser & Normalizer\n",
    "\n",
    "### **Main Objective**\n",
    "To transform the raw, unstructured, and nested JSON data acquired by the crawler into structured, relational CSV files suitable for database ingestion (specifically Neo4j) and analysis.\n",
    "\n",
    "### **Technical Logic**\n",
    "This script acts as an ETL (Extract, Transform, Load) pipeline, converting a file-system-based storage into a tabular format:\n",
    "\n",
    "* **Hierarchical Traversal:** The script recursively iterates through the directory structure created by the crawler (`State` → `Volumes` → `Cases`), ensuring that metadata files (`ReporterMetadata`, `VolumesMetadata`) are linked to the correct cases.\n",
    "* **Schema Flattening:** It parses the complex, nested JSON structure of the Case.law schema. It extracts specific fields (e.g., jurisdiction, court, decision date) and flattens nested lists (like judges, parties, or attorneys) into pipe-separated strings (`|`) to fit into a flat CSV format.\n",
    "* **Data Normalization & Separation:** To optimize performance and database design, the script splits the data into four distinct logical entities:\n",
    "    1.  `state_reports.csv`: General metadata about the reporter.\n",
    "    2.  `volumes_metadata.csv`: Details about specific book volumes.\n",
    "    3.  `cases.csv`: Lightweight metadata for each case (titles, dates, parties, citations), allowing for fast indexing.\n",
    "    4.  `case_texts.csv`: The \"heavy\" payload containing the full text of opinions and headnotes, separated to avoid bloating the main node properties in the graph.\n",
    "* **Memory Optimization:** Data is collected in standard Python lists of dictionaries and converted to **Pandas DataFrames** only at the end for efficient batch writing to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14288d0a-5215-4a02-9bef-d597fe906f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"Test/case_law_json\"  # Root folder containing state folders\n",
    "OUTPUT_DIR = \"Test/case_law_csv\"\n",
    "STATE_REPORTS_CSV = \"state_reports.csv\"\n",
    "VOLUMES_METADATA_CSV = \"volumes_metadata.csv\"\n",
    "CASES_CSV = \"cases.csv\"\n",
    "CASE_TEXTS_CSV = \"case_texts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4b5b54-03be-4930-a9ed-2068d0422474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring output directory exists: Test/case_law_csv\n",
      "Starting parsing from root: Test/case_law_json\n",
      "Processing state folder: tex-crim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing tex-crim volumes: 100%|██████████| 142/142 [08:22<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving CSV files to Test/case_law_csv...\n",
      "\n",
      "--- Parsing complete ---\n",
      "Total cases processed: 27712\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "# --- CONFIGURAZIONE PERCORSI ---\n",
    "# ROOT_DIR deve puntare alla cartella che CONTIENE le cartelle degli stati (es. 'tex-crim')\n",
    "ROOT_DIR = \"Test/case_law_json\"  \n",
    "\n",
    "# OUTPUT_DIR è dove verranno salvati i CSV. Verrà creata automaticamente.\n",
    "OUTPUT_DIR = \"Test/case_law_csv\" \n",
    "# --- FINE CONFIGURAZIONE ---\n",
    "\n",
    "# Definisce i nomi dei file di output\n",
    "STATE_REPORTS_CSV = \"state_reports.csv\"\n",
    "VOLUMES_METADATA_CSV = \"volumes_metadata.csv\"\n",
    "CASES_CSV = \"cases.csv\"\n",
    "CASE_TEXTS_CSV = \"case_texts.csv\"\n",
    "\n",
    "# Liste globali per raccogliere i dati\n",
    "state_reports_data = []\n",
    "volumes_metadata_data = []\n",
    "cases_data = []\n",
    "case_texts_data = []\n",
    "\n",
    "def process_state_folder(state_folder_path):\n",
    "    \"\"\"Processa una singola cartella di stato (es. 'tex-crim').\"\"\"\n",
    "    slug = os.path.basename(state_folder_path)\n",
    "    print(f\"Processing state folder: {slug}\")\n",
    "    \n",
    "    # --- Process ReporterMetadata.json ---\n",
    "    reporter_file = os.path.join(state_folder_path, \"ReporterMetadata.json\")\n",
    "    if os.path.exists(reporter_file):\n",
    "        try:\n",
    "            with open(reporter_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                reporter = json.load(f)\n",
    "            jurisdiction = reporter.get(\"jurisdictions\", [{}])[0]\n",
    "            state_reports_data.append({\n",
    "                \"id\": reporter.get(\"id\"),\n",
    "                \"full_name\": reporter.get(\"full_name\"),\n",
    "                \"short_name\": reporter.get(\"short_name\"),\n",
    "                \"start_year\": reporter.get(\"start_year\"),\n",
    "                \"end_year\": reporter.get(\"end_year\"),\n",
    "                \"jurisdictions_id\": jurisdiction.get(\"id\"),\n",
    "                \"jurisdictions_name\": jurisdiction.get(\"name\"),\n",
    "                \"jurisdictions_name_long\": jurisdiction.get(\"name_long\"),\n",
    "                \"slug\": reporter.get(\"slug\")\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {reporter_file}: {e}\")\n",
    "\n",
    "    # --- Process VolumesMetadata.json ---\n",
    "    volumes_file = os.path.join(state_folder_path, \"VolumesMetadata.json\")\n",
    "    if os.path.exists(volumes_file):\n",
    "        try:\n",
    "            with open(volumes_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                volumes = json.load(f)\n",
    "            for v in volumes:\n",
    "                jurisdiction = v.get(\"jurisdictions\", [{}])[0]\n",
    "                nominative = v.get(\"nominative_reporter\") or {}\n",
    "                volumes_metadata_data.append({\n",
    "                    \"volume_number\": v.get(\"volume_number\"),\n",
    "                    \"title\": v.get(\"title\"),\n",
    "                    \"publisher\": v.get(\"publisher\"),\n",
    "                    \"publication_year\": v.get(\"publication_year\"),\n",
    "                    \"start_year\": v.get(\"start_year\"),\n",
    "                    \"end_year\": v.get(\"end_year\"),\n",
    "                    \"series_volume_number\": v.get(\"series_volume_number\"),\n",
    "                    \"jurisdictions_id\": jurisdiction.get(\"id\"),\n",
    "                    \"jurisdictions_name\": jurisdiction.get(\"name\"),\n",
    "                    \"jurisdictions_name_long\": jurisdiction.get(\"name_long\"),\n",
    "                    \"id\": v.get(\"id\"),\n",
    "                    \"harvard_hollis_id\": v.get(\"harvard_hollis_id\"),\n",
    "                    \"spine_start_year\": v.get(\"spine_start_year\"),\n",
    "                    \"spine_end_year\": v.get(\"spine_end_year\"),\n",
    "                    \"publication_city\": v.get(\"publication_city\"),\n",
    "                    \"redacted\": v.get(\"redacted\"),\n",
    "                    \"nominative_volume_number\": nominative.get(\"volume_number\"),\n",
    "                    \"nominative_name\": nominative.get(\"nominative_name\"),\n",
    "                    \"volume_folder\": v.get(\"volume_folder\"),\n",
    "                    \"reporter_slug\": v.get(\"reporter_slug\")\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {volumes_file}: {e}\")\n",
    "\n",
    "    # --- Process cases inside volume folders ---\n",
    "    # Itera su tutte le cartelle dei volumi (es. '1', '2', '3'...)\n",
    "    if not os.path.exists(state_folder_path):\n",
    "         print(f\"Warning: Folder {state_folder_path} does not exist.\")\n",
    "         return\n",
    "\n",
    "    # Ottieni la lista delle cartelle, ignorando file sciolti\n",
    "    volume_folders = [f for f in os.listdir(state_folder_path) if os.path.isdir(os.path.join(state_folder_path, f))]\n",
    "\n",
    "    for volume_folder in tqdm(volume_folders, desc=f\"Parsing {slug} volumes\"):\n",
    "        volume_folder_path = os.path.join(state_folder_path, volume_folder)\n",
    "        \n",
    "        # I casi si trovano in una sottocartella 'cases'\n",
    "        cases_json_dir = os.path.join(volume_folder_path, \"cases\")\n",
    "        \n",
    "        # Controlla se la cartella 'cases' esiste\n",
    "        if not os.path.isdir(cases_json_dir):\n",
    "            continue \n",
    "\n",
    "        # Itera su tutti i file JSON *dentro* la cartella 'cases'\n",
    "        for filename in os.listdir(cases_json_dir):\n",
    "            if not filename.endswith(\".json\"):\n",
    "                continue\n",
    "                \n",
    "            case_file_path = os.path.join(cases_json_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(case_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    case_data = json.load(f)\n",
    "\n",
    "                if isinstance(case_data, list):\n",
    "                    case_list = case_data\n",
    "                elif isinstance(case_data, dict):\n",
    "                    case_list = [case_data]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                for case in case_list:\n",
    "                    if not isinstance(case, dict):\n",
    "                        continue\n",
    "\n",
    "                    court = case.get(\"court\") or {}\n",
    "                    jurisdiction = case.get(\"jurisdiction\") or {}\n",
    "                    casebody = case.get(\"casebody\", {})\n",
    "                    \n",
    "                    # --- cases.csv ---\n",
    "                    cases_data.append({\n",
    "                        \"id\": case.get(\"id\"),\n",
    "                        \"name\": case.get(\"name\"),\n",
    "                        \"name_abbreviation\": case.get(\"name_abbreviation\"),\n",
    "                        \"decision_date\": case.get(\"decision_date\"),\n",
    "                        \"docket_number\": case.get(\"docket_number\"),\n",
    "                        \"first_page\": case.get(\"first_page\"),\n",
    "                        \"last_page\": case.get(\"last_page\"),\n",
    "                        \"court_id\": court.get(\"id\"),\n",
    "                        \"court_name\": court.get(\"name\"),\n",
    "                        \"court_name_abbreviation\": court.get(\"name_abbreviation\"),\n",
    "                        \"jurisdiction_id\": jurisdiction.get(\"id\"),\n",
    "                        \"jurisdiction_name\": jurisdiction.get(\"name\"),\n",
    "                        \"jurisdiction_name_long\": jurisdiction.get(\"name_long\"),\n",
    "                        \"judges\": \"|\".join(casebody.get(\"judges\", []) or []),\n",
    "                        \"parties\": \"|\".join(casebody.get(\"parties\", []) or []),\n",
    "                        \"attorneys\": \"|\".join(casebody.get(\"attorneys\", []) or []),\n",
    "                        \"state_volume_file\": f\"/{slug}/{volume_folder}/cases/{filename.replace('.json','')}\"\n",
    "                    })\n",
    "\n",
    "                    # --- case_texts.csv ---\n",
    "                    opinions = casebody.get(\"opinions\", []) or []\n",
    "                    opinions_text = \"|\".join([op.get(\"text\",\"\") for op in opinions if op])\n",
    "                    \n",
    "                    case_texts_data.append({\n",
    "                        \"id\": case.get(\"id\"),\n",
    "                        \"title\": case.get(\"name\"),\n",
    "                        \"head_matter\": casebody.get(\"head_matter\", \"\"),\n",
    "                        \"corrections\": casebody.get(\"corrections\", \"\"),\n",
    "                        \"opinions\": opinions_text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading case {case_file_path}: {e}\")\n",
    "\n",
    "# --- Blocco di esecuzione principale ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. CREA LA CARTELLA DI OUTPUT (Così non devi farlo a mano)\n",
    "    print(f\"Ensuring output directory exists: {OUTPUT_DIR}\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting parsing from root: {ROOT_DIR}\")\n",
    "    \n",
    "    if os.path.exists(ROOT_DIR):\n",
    "        # Cerca cartelle dentro ROOT_DIR (es. 'tex-crim')\n",
    "        for state_folder in os.listdir(ROOT_DIR):\n",
    "            state_folder_path = os.path.join(ROOT_DIR, state_folder)\n",
    "            if os.path.isdir(state_folder_path):\n",
    "                process_state_folder(state_folder_path)\n",
    "    else:\n",
    "        print(f\"ERROR: Root directory '{ROOT_DIR}' not found!\")\n",
    "\n",
    "    # --- Salvataggio ---\n",
    "    if cases_data:\n",
    "        print(f\"\\nSaving CSV files to {OUTPUT_DIR}...\")\n",
    "        pd.DataFrame(state_reports_data).to_csv(os.path.join(OUTPUT_DIR, STATE_REPORTS_CSV), index=False)\n",
    "        pd.DataFrame(volumes_metadata_data).to_csv(os.path.join(OUTPUT_DIR, VOLUMES_METADATA_CSV), index=False)\n",
    "        pd.DataFrame(cases_data).to_csv(os.path.join(OUTPUT_DIR, CASES_CSV), index=False)\n",
    "        pd.DataFrame(case_texts_data).to_csv(os.path.join(OUTPUT_DIR, CASE_TEXTS_CSV), index=False)\n",
    "\n",
    "        print(\"\\n--- Parsing complete ---\")\n",
    "        print(f\"Total cases processed: {len(cases_data)}\")\n",
    "    else:\n",
    "        print(\"\\nNo cases found to save. Check your paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cfe4a",
   "metadata": {},
   "source": [
    "## Citation Network Extractor\n",
    "\n",
    "### **Main Objective**\n",
    "To build the \"Who Cites Whom\" graph by parsing the citation metadata embedded within the downloaded JSON case files and resolving references between cases.\n",
    "\n",
    "### **Technical Logic**\n",
    "This script is crucial for creating the edges (relationships) in the graph database. Its logic focuses on resolving entities:\n",
    "\n",
    "* **Source Identification:** It iterates through all available JSON files again. For each case, it extracts its unique ID, which serves as the source node (`id1`) of the relationship.\n",
    "* **Target Resolution (The Matching Problem):** The script analyzes the `cites_to` field in the JSON, which contains outgoing citations. It uses a multi-tiered strategy to link these citations to other cases in the database:\n",
    "    1.  **Explicit ID Match:** It first checks if the cited case has a direct numerical ID (`case_ids`). If this ID exists in the set of previously downloaded cases (`id_set`), a verified link is created.\n",
    "    2.  **Path Match:** If no ID is available, it attempts to resolve the citation using the file path (`case_paths`), matching it against a pre-computed dictionary mapping file paths to IDs.\n",
    "    3.  **Unresolved Citations:** If a citation points to a case outside the dataset (e.g., a Civil case or a non-Texas case), it is still recorded but flagged as `matched=False`. This preserves the citation text for display without creating a dangling edge in the graph.\n",
    "* **Data Integrity:** Before processing, it loads the `cases.csv` file created by the previous script to build a whitelist of valid IDs. This ensures referential integrity: the script only creates relationships where the source node is guaranteed to exist in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f0b419-6e0b-42e4-a3e5-45eaac86b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cases from Test/case_law_csv/cases.csv...\n",
      "Loaded 27712 case IDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing states: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 191572 citations to Test/case_law_csv/citations.csv...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "# Deve corrispondere a dove hai i JSON e i CSV del Texas\n",
    "ROOT_DIR = \"Test/case_law_json\"\n",
    "CASES_CSV = \"Test/case_law_csv/cases.csv\"\n",
    "CITATIONS_CSV = \"Test/case_law_csv/citations.csv\"\n",
    "\n",
    "print(f\"Loading cases from {CASES_CSV}...\")\n",
    "# Load cases.csv\n",
    "# Usiamo dtype str per evitare problemi con ID numerici\n",
    "cases_df = pd.read_csv(CASES_CSV, dtype={\"id\": str})\n",
    "\n",
    "# Creiamo una mappa per trovare velocemente l'ID dato il percorso del file\n",
    "# Importante: Il percorso nel CSV è tipo \"/tx-crim/1/cases/0001\"\n",
    "file_to_id = dict(zip(cases_df[\"state_volume_file\"], cases_df[\"id\"]))\n",
    "id_set = set(cases_df[\"id\"])\n",
    "\n",
    "print(f\"Loaded {len(id_set)} case IDs.\")\n",
    "\n",
    "citations_data = []\n",
    "\n",
    "# --- INIZIO ELABORAZIONE ---\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    print(f\"ERROR: Root directory {ROOT_DIR} not found.\")\n",
    "    exit()\n",
    "\n",
    "# Itera sugli stati (es. 'tx-crim')\n",
    "state_folders = [f for f in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "\n",
    "for state_folder in tqdm(state_folders, desc=\"Processing states\"):\n",
    "    state_folder_path = os.path.join(ROOT_DIR, state_folder)\n",
    "    \n",
    "    # Itera sui volumi\n",
    "    volume_folders = [f for f in os.listdir(state_folder_path) if os.path.isdir(os.path.join(state_folder_path, f))]\n",
    "    \n",
    "    for volume_folder in tqdm(volume_folders, desc=f\"Volumes in {state_folder}\", leave=False):\n",
    "        volume_folder_path = os.path.join(state_folder_path, volume_folder)\n",
    "        \n",
    "        # --- MODIFICA IMPORTANTE: Cerca nella sottocartella 'cases' ---\n",
    "        cases_dir = os.path.join(volume_folder_path, \"cases\")\n",
    "        if not os.path.isdir(cases_dir):\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(cases_dir):\n",
    "            if not filename.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(cases_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    case_json = json.load(f)\n",
    "            except (json.JSONDecodeError, OSError) as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Normalize to list (gestisce sia singolo oggetto che lista di oggetti)\n",
    "            if isinstance(case_json, dict):\n",
    "                case_list = [case_json]\n",
    "            elif isinstance(case_json, list):\n",
    "                case_list = case_json\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for case in case_list:\n",
    "                # ID del caso CHE CITA (Source)\n",
    "                id1 = str(case.get(\"id\"))\n",
    "                \n",
    "                # Se questo caso non è nel nostro CSV (es. errore di parsing precedente), lo saltiamo\n",
    "                if id1 not in id_set:\n",
    "                    continue\n",
    "\n",
    "                # Costruiamo il percorso \"virtuale\" per il matching.\n",
    "                # Deve essere identico a quello generato in json_to_csv_parser.py:\n",
    "                # /{slug}/{volume}/cases/{filename_no_ext}\n",
    "                filename_no_ext = filename.replace('.json','')\n",
    "                # Nota: aggiungiamo lo slash iniziale perché nel CSV c'è\n",
    "                current_state_volume_file = f\"/{state_folder}/{volume_folder}/cases/{filename_no_ext}\"\n",
    "\n",
    "                # Processiamo le citazioni (OUTGOING edges)\n",
    "                cites_to = case.get(\"cites_to\", [])\n",
    "                if not cites_to:\n",
    "                    continue\n",
    "\n",
    "                for cite in cites_to:\n",
    "                    citation_reference = cite.get(\"cite\")\n",
    "                    category = cite.get(\"category\")\n",
    "                    reporter = cite.get(\"reporter\")\n",
    "                    opinion_index = cite.get(\"opinion_index\")\n",
    "\n",
    "                    match_found = False\n",
    "\n",
    "                    # --- Strategia 1: Link tramite case_ids (Esplicito) ---\n",
    "                    for id2 in cite.get(\"case_ids\", []):\n",
    "                        id2_str = str(id2)\n",
    "                        \n",
    "                        # Determiniamo se è un \"match\" interno (il caso citato è nel nostro DB)\n",
    "                        is_matched = id2_str in id_set\n",
    "                        \n",
    "                        citations_data.append({\n",
    "                            \"id1\": id1,\n",
    "                            \"id2\": id2_str,\n",
    "                            \"citation_reference\": citation_reference,\n",
    "                            \"category\": category,\n",
    "                            \"reporter\": reporter,\n",
    "                            \"opinion_index\": opinion_index,\n",
    "                            \"matched\": is_matched\n",
    "                        })\n",
    "                        match_found = True\n",
    "\n",
    "                    # --- Strategia 2: Link tramite case_paths (Implicito) ---\n",
    "                    # Utile se case_ids è vuoto ma abbiamo un percorso file\n",
    "                    if not match_found: # Evitiamo duplicati se abbiamo già trovato via ID\n",
    "                        for path in cite.get(\"case_paths\", []):\n",
    "                            # path è spesso relativo o parziale, proviamo a vedere se lo abbiamo mappato\n",
    "                            # Nota: case.law paths possono variare, questo è un tentativo best-effort\n",
    "                            id2_str = file_to_id.get(path)\n",
    "                            \n",
    "                            if id2_str:\n",
    "                                citations_data.append({\n",
    "                                    \"id1\": id1,\n",
    "                                    \"id2\": id2_str,\n",
    "                                    \"citation_reference\": citation_reference,\n",
    "                                    \"category\": category,\n",
    "                                    \"reporter\": reporter,\n",
    "                                    \"opinion_index\": opinion_index,\n",
    "                                    \"matched\": True\n",
    "                                })\n",
    "                                match_found = True\n",
    "\n",
    "                    # --- Strategia 3: Nessun ID trovato (Citazione esterna o non risolta) ---\n",
    "                    # Se non abbiamo trovato né ID né Path, registriamo comunque la citazione\n",
    "                    # ma con id2=None e matched=False. Utile per statistiche.\n",
    "                    if not match_found:\n",
    "                         citations_data.append({\n",
    "                            \"id1\": id1,\n",
    "                            \"id2\": None,\n",
    "                            \"citation_reference\": citation_reference,\n",
    "                            \"category\": category,\n",
    "                            \"reporter\": reporter,\n",
    "                            \"opinion_index\": opinion_index,\n",
    "                            \"matched\": False\n",
    "                        })\n",
    "\n",
    "# Save citations table\n",
    "print(f\"Saving {len(citations_data)} citations to {CITATIONS_CSV}...\")\n",
    "if citations_data:\n",
    "    df_citations = pd.DataFrame(citations_data)\n",
    "    # Rimuoviamo eventuali duplicati esatti\n",
    "    df_citations = df_citations.drop_duplicates()\n",
    "    df_citations.to_csv(CITATIONS_CSV, index=False)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"No citations found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
